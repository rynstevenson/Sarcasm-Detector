{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bab46aa",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd897c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "import random\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"All libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ccad5",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "Gathered a bunch of datasets because they've been failing. Will select the first one that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df2a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try multiple datasets in order of preference\n",
    "dataset = None\n",
    "dataset_name = None\n",
    "\n",
    "datasets_to_try = [\n",
    "    (\"raquiba/Sarcasm_News_Headline\", \"News Headlines Sarcasm Dataset\"),\n",
    "    (\"helinivan/english-sarcasm-detector\", \"English Sarcasm Detector Dataset\"),\n",
    "    (\"siddhant4583agarwal/sarcasm-detection-dataset\", \"Sarcasm Detection Dataset\")\n",
    "]\n",
    "\n",
    "for dataset_id, description in datasets_to_try:\n",
    "    try:\n",
    "        print(f\"Trying to load {description}...\")\n",
    "        dataset = load_dataset(dataset_id)\n",
    "        dataset_name = description\n",
    "        print(f\"{description} loaded successfully!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {description}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Fallback: make synthetic dataset if nothing works\n",
    "if dataset is None:\n",
    "    print(\"None of the datasets worked. Making a synthetic one.\")\n",
    "    synthetic_data = {\n",
    "        \"headline\": [\n",
    "            \"Area Man Constantly Mentioning He Doesn't Own a Television\",\n",
    "            \"Local Woman Takes Up Jogging, Tells Everyone\",\n",
    "            \"Breaking: Rain Causes Things to Get Wet\",\n",
    "            \"Scientists Discover Fire Hot, Water Wet\",\n",
    "            \"Man Who Plays Guitar at Parties Loses Guitar Privileges\",\n",
    "            \"President Signs Bill Into Law\",\n",
    "            \"New Study Shows Exercise Beneficial for Health\"\n",
    "        ],\n",
    "        \"is_sarcastic\": [1, 1, 1, 1, 1, 0, 0]\n",
    "    }\n",
    "    dataset = {\"train\": Dataset.from_dict(synthetic_data)}\n",
    "    dataset_name = \"Synthetic Dataset\"\n",
    "    print(\"Synthetic dataset created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443990e",
   "metadata": {},
   "source": [
    "## Split the dataset\n",
    "Some potential datasets have premade train/test splits, so we check for that. If not, we make one ourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which part of the dataset to use\n",
    "if 'train' in dataset:\n",
    "    main_data = dataset['train']\n",
    "elif 'test' in dataset:\n",
    "    main_data = dataset['test']\n",
    "else:\n",
    "    main_data = dataset\n",
    "\n",
    "print(f\"Using: {dataset_name}\")\n",
    "print(f\"Dataset size: {len(main_data)}\")\n",
    "\n",
    "# If dataset is really big, just take a subset\n",
    "if len(main_data) > 50000:\n",
    "    print(\"Using a 20k sample for speed.\")\n",
    "    main_data = main_data.shuffle(seed=42).select(range(20000))\n",
    "\n",
    "# Make train/test split\n",
    "dataset = main_data.train_test_split(test_size=0.2, seed=42)\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a8c6d",
   "metadata": {},
   "source": [
    "## Preprocess the text\n",
    "Data cleaning/preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    df = dataset.to_pandas()\n",
    "    text_col = next((c for c in ['text', 'comment', 'headline', 'sentence', 'content'] if c in df.columns), None)\n",
    "    label_col = next((c for c in ['label', 'labels', 'sarcastic', 'is_sarcastic', 'target'] if c in df.columns), None)\n",
    "\n",
    "    if text_col is None:\n",
    "        raise ValueError(\"Can't find a text column.\")\n",
    "\n",
    "    if label_col is None:\n",
    "        print(\"No label col found â€” generating fake labels.\")\n",
    "        df['labels'] = df[text_col].str.contains(r'\\b(great|awesome|fantastic|perfect)\\b', case=False).astype(int)\n",
    "    else:\n",
    "        df = df.dropna(subset=[text_col, label_col])\n",
    "        df['labels'] = df[label_col].astype(int)\n",
    "\n",
    "    df['text'] = df[text_col].apply(clean_text)\n",
    "    df = df[df['text'].str.len() > 0]\n",
    "    return df[['text', 'labels']]\n",
    "\n",
    "train_df = preprocess_dataset(dataset['train'])\n",
    "test_df = preprocess_dataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae3b3af",
   "metadata": {},
   "source": [
    "## Tokenization and formatting\n",
    "Tokenization + prepping for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['text'].tolist(),\n",
    "    train_df['labels'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['labels']\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'labels': train_labels})\n",
    "val_dataset = Dataset.from_dict({'text': val_texts, 'labels': val_labels})\n",
    "test_dataset = Dataset.from_dict({'text': test_df['text'].tolist(), 'labels': test_df['labels'].tolist()})\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "for ds in [train_dataset, val_dataset, test_dataset]:\n",
    "    ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b7eb2e",
   "metadata": {},
   "source": [
    "## Model setup with LoRA\n",
    "Choosing LoRA for PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"not_sarcastic\", 1: \"sarcastic\"},\n",
    "    label2id={\"not_sarcastic\": 0, \"sarcastic\": 1}\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86d502",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=[],\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d2e78",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cb5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d5dfc2",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Not Sarcastic', 'Sarcastic'],\n",
    "            yticklabels=['Not Sarcastic', 'Sarcastic'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb015afb",
   "metadata": {},
   "source": [
    "## Try some sample inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sarcasm(text, model, tokenizer):\n",
    "    text = clean_text(text)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1).item()\n",
    "        conf = probs[0][pred].item()\n",
    "    return pred, conf\n",
    "\n",
    "examples = [\n",
    "    \"How about that.\",\n",
    "    \"The weather is really nice today.\",\n",
    "    \"I love when my phone dies!\"\n",
    "]\n",
    "\n",
    "for i, ex in enumerate(examples):\n",
    "    label, conf = predict_sarcasm(ex, model, tokenizer)\n",
    "    print(f\"{i+1}. {ex} --> {'Sarcastic' if label==1 else 'Not Sarcastic'} ({conf:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36235a90",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./sarcasm_detector_lora\")\n",
    "tokenizer.save_pretrained(\"./sarcasm_detector_lora\")\n",
    "print(\"Saved!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
